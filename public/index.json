
[{"content":"Hi! I\u0026rsquo;m Denis Wambold and I\u0026rsquo;m currently doing my Master\u0026rsquo;s in Computer Science at Karlsruhe Institute of Technology (KIT). I\u0026rsquo;m particularly interested in offensive and defensive cybersecurity üë®‚Äçüíª and artificial intelligence ü§ñ.\nIn case you want to have a talk, feel free to reach out to me on whatever platform you prefer! :)\n","date":"1 December 2024","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"1 December 2024","externalUrl":null,"permalink":"/adventskalender/","section":"Adventskalender","summary":"","title":"Adventskalender","type":"adventskalender"},{"content":"Hallo Arinna,\nda du gerade in Kanada bist, ist der Adventskalender dieses Jahr etwas anders! Der heutige Tag dreht sich, passend zum eisigen Winter in Kanada, um die K√§lte.\nUnd wer mag die K√§lte besonders??? Zwei sehr wichtige Familienmitglieder der Wambolds!!!\nGerade in Kanada sind Huskies ein wichtiger Teil der Gesellschaft. Je weiter es in den Norden geht, umso wichter werden die s√º√üen Hunde. Nicht nur als Kraftmaschinen f√ºr Schlitten, sondern auch als Familienmitglieder, Freunde, und vor allem: Besch√ºtzer.\nSie besch√ºtzen die Menschen vor dem Hunger, vor Langeweile und\u0026hellip; vor Eisb√§ren!\nHier ein cooler Beitrag √ºber Natives im Norden Kanadas und wie sie zusammen mit ihrem Husky-Rudel auf Jadg gehen:\nUnd hier ein kurzes Gedicht:\nHuskies im Winterwald\nIm Winterwald, so tief verschneit, ziehen Huskies durch die wei√üe Zeit. Mit Augen klar wie ein eisiger See, gleiten sie lautlos durch den Schnee. Ihr Atem malt Wolken in kalter Luft, ihr Fell tr√§gt den Geruch von Tannenduft. Schwarz-wei√ü die eine, wie Nacht und Licht, die andere bronzen, ein warmes Gesicht. Sie jagen durch die frostige Nacht, bewachen die Sterne, bis der Morgen erwacht. Ihr Heulen klingt wie ein winterliches Lied, das die K√§lte mit Melodie durchzieht. Unterm Weihnachtsbaum, in friedlicher Ruh‚Äô, schmiegen sie sich an, die Augen zu. Vom Schnee in den Tr√§umen, vom Mondlicht erhellt, tragen sie die Magie der arktischen Welt. Oh Huskies, H√ºter der Weihnachtszeit, ihr bringt uns Freude und Herzlichkeit. Mit eurem L√§cheln, so treu und klar, seid ihr die Sterne, dem Herzen so nah. ","date":"1 December 2024","externalUrl":null,"permalink":"/adventskalender/t%C3%BCrchen1/","section":"Adventskalender","summary":"Huskies im Winterwald","title":"T√ºrchen Nummer 1","type":"adventskalender"},{"content":"Aloha Fr√§ulein,\nneben Huskies spielen auch andere Tiere eine gro√üe Rolle in deinem Leben. Auch wenn das vielleicht nicht immer gewollt ist, und du sie manchmal verfluchen k√∂nntest, so sind sie doch auch ein bisschen s√º√ü.\nRichtig, es geht um Einch√∂rnchen. Sie kommen in verschiedenen Farben und Gr√∂√üen, also habe ich hier ein kleines Video rausgesucht, dass du deine Zimmernachbarn besser kennenlernst!\nEinch√∂rnchen auf dem Dach, das macht Krack\nAuf einem Dach, verschneit und kalt, lebt eine Familie, quirlig und bald, zwei gro√üe Eichh√∂rnchen und Babys drei, turnen herum, voller Lust und Geschrei. Darunter wohnt Arinna, ganz keck, in ihrem Zimmer, gleich unterm Eck. Jeden Morgen h√∂rt sie tapsige Schritte, und denkt sich: \u0026#34;Schon wieder, die kleine Bande Dritte!\u0026#34; \u0026#34;Sie sammeln N√ºsse,\u0026#34; murmelt sie leise, \u0026#34;und veranstalten dabei eine Polonaise. Vom Schornstein bis runter zur Dachrinne, toben die Eichh√∂rnchen wie kleine Spinner!\u0026#34; Doch eines Morgens, da wird es bunt, als ein Baby-Eichh√∂rnchen purzelt auf den Grund. Arinna eilt raus mit Schal und Schuh, und ruft: \u0026#34;Kleiner Freund, was machst nur du?\u0026#34; Das Baby piepst leise, ganz klein und nass, Arinna hebt es auf: \u0026#34;Du bist ein Spa√ü! Zur√ºck aufs Dach, zu deiner verr√ºckten Bande, damit ich hier endlich mal Ruhe f√§nde!\u0026#34; Doch nachts, da tr√§umt sie, von tapsigen Schritten, und denkt: \u0026#34;Eigentlich mag ich die kleinen Fritten. Die Welt w√§re still, ohne ihr lustiges Leben, sollen sie doch weiter aufs Dach hier beben!\u0026#34; So lebt Arinna, mit ihrer quirlig\u0026#39;n Crew, und lacht jeden Morgen √ºber das Dach-Turnier zu. Denn manchmal, da ist‚Äôs, das muss man gesteh‚Äôn, die kleinen Nervens√§gen, die Welt sch√∂n zu seh‚Äôn! ","date":"1 December 2024","externalUrl":null,"permalink":"/adventskalender/t%C3%BCrchen2/","section":"Adventskalender","summary":"Huskies im Winterwald","title":"T√ºrchen Nummer 1","type":"adventskalender"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/blog/","section":"Blog","summary":"","title":"Blog","type":"blog"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/gan/","section":"Tags","summary":"","title":"Gan","type":"tags"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/outlier-detection/","section":"Tags","summary":"","title":"Outlier Detection","type":"tags"},{"content":" Subspace generative\u0026hellip; Subspace Generative Adversarial Learning for Unsupervised Outlier Detection! # This was the title of my Bachelor\u0026rsquo;s Thesis. At first glance, it might seem like this is just one big buzzword bingo. However, if we explore the work done, we quickly notice that it actually describes the thesis well\u0026hellip; Now, let\u0026rsquo;s dive right into what I did and why this seemingly buzzy title is not so buzzy afterall.\nThe Idea # In recent years, deep generative methods have enabled several subfields of Machine Learning to make great progress. Previously overshadowed by supervised methods, these unsupervised methods now thrive as they yield impressive performance without the need of labels. Particularly, the introduction of Generative Adversarial Networks (GAN) and their game-theoretical approach to Deep Learning proved how powerful generative methods are. GANs manage to generate realistic data, thus they are suited for many different Machine Learning tasks such as Novelty Detection or Image Generation. Especially for high-dimensional data, GANs impress with their performance. Over the years, several extensions to the GAN framework have been published, leveraging the generative nature of the model to detect outliers. While there are models like MO-GAAL, which embed an ensemble structure to GANs to generate outliers, or AnoGAN, which works with a latent representation of data to detect outliers, all of these GAN extensions still lack one property: subspace Search. In high-dimensional spaces, outliers are sometimes only, or better, visible in some feature subspaces, making it hard to find them when only looking at the full set of features.\nThat is why we want to combine the generative strength of GANs with a feature subspace ensemble to tackle this missed opportunity. To achieve this, we have to solve a few issues:\nRedesign the model architecture Redesign the gametheoretical gnerative approach Handle feature dependencies With FeGAN, the model proposed by us, we manage to utilize the generative nature of GANs to learn the distribution of multiple feature subspaces. This allows to look for outliers in these subspaces respectively and therefore achieve a higher success rate in Outlier Detection.\nGenerative Adversarial Networks # GANs typically consist of two neural networks: A Generator G and a Discriminator D. Each of those two parties has one unique goal. The generator aims to, almost, approximate the underlying data distribution. Ideally, G is able to generate realistic data samples at the end of the training. Its counterpart, the Discriminator, doesn\u0026rsquo;t really care about the underlying distribution as much. D\u0026rsquo;s actual goal is quite simple as it is to distinguish between real data samples and the ones generated by G.\nOne can imagine the GAN training procedure as a feud between an art forger and an art expert. While the art expert\u0026rsquo;s job is to be really good at spotting fake art, the art forger tries to trick the expert. During the game, the art forger becomes better and better at faking artwork, while, at some point, the art expert might not be able to distinguish between real and fake artwork anymore.\nThis training results in a minimax (zero-sum) game, which is held by two parties.\nIf you\u0026rsquo;re interested in the deeper maths behind this model or other details, please refer to the original GAN paper.\nFeature Ensemble GAN - FeGAN # The first step to solve our task is to adjust the model architecture and think about how we can incorporate multiple subspaces into the training process. A very naive approach would be to simply use multiple GANs, each training on its own, unique, subspace. However, that doesn\u0026rsquo;t help us much as each Generator would only be able to generate samples from that subspace. Therefore, we would not approximate the full sample space and probably lose important feature dependencies. So, what do we do now? How about we only use one Generator, that trains on the full feature space? And then we project the generated full dimensional samples down to specific subspaces? That way, we still generate on the complete feature space while also enabling the Discriminators to work on lower-dimensional subspaces. Of course, this is only possible if the Discriminators work with lower-dimensional datasets to begin with.\nThat second idea is what we chose for FeGAN. We use one Generator working on the full feature space, and let it interact with N Discriminators, each training on their own unique subspace. To classify whether a sample is an outlier or not, we let each Discriminator decide and then use the average of all decisions.\nThe adjusted architecture already gives a good direction on how the target function and the minimax game of the original GAN have to be adjusted in order to work. However, I choose not to go into detail here, as it is plain math. Since I\u0026rsquo;ve already written my thesis, you can simple head there and look it up if you are interested (hint: you can also look here)!\nSomething not so clear is the selection of \u0026ldquo;good\u0026rdquo; subspaces. Ideally, we want subspaces to be informative and independent to prevent redundancy. Unfortunately, due to the exponential number of possible subspaces, choosing the \u0026ldquo;good\u0026rdquo; ones, is very hard. While there are certain algorithms for this, they all have their pros and cons and we chose to stick to the most simple method: Random selection. Admittedly, this may not be the best choice in terms of prediction accuracy, but it is sufficient to prove the functionality of the model.\nConclusion # In my thesis, we extended the vanilla GAN architecture to create FeGAN. We combine one Generator with multiple Discriminators, each working on their on unique feature subspace to form one ensemble, improving prediction quality. Furthermore, we adjusted the training process, the zero-sum game and the target function, to make the original GAN training compatible with our novel adjustments.\nThe code for FeGAN (old name, now rebranded to GSAAL) can be found on my GitHub.\nFinal words \u0026amp; Future # We were really satisfied with the results of FeGAN. That\u0026rsquo;s why we\u0026rsquo;ve decided to continue researching the model and write a paper about it! At the moment, we are in the reviewing process of a major conference, but a pre-print version is already available on arxiv.\nGenerative Subspace Adversarial Active Learning for Outlier Detection in Multiple Views of High-dimensional Data\n","date":"1 June 2024","externalUrl":null,"permalink":"/blog/fegan/","section":"Blog","summary":"A condensed summary of my Bachelor\u0026rsquo;s Thesis!","title":"Subspace generative... what? ELI5","type":"blog"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/subspaces/","section":"Tags","summary":"","title":"Subspaces","type":"tags"},{"content":"","date":"1 June 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"1 May 2024","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]